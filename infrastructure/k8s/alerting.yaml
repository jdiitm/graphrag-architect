apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: graphrag-alerts
  namespace: graphrag
  labels:
    prometheus: kube-prometheus
    role: alert-rules
spec:
  groups:
    - name: graphrag.slo.recording
      interval: 30s
      rules:
        - record: graphrag:query_success_rate:5m
          expr: >
            1 - (
              sum(rate(http_server_request_duration_seconds_count{route="/query",status_code=~"5.."}[5m]))
              /
              clamp_min(sum(rate(http_server_request_duration_seconds_count{route="/query"}[5m])), 1)
            )

        - record: graphrag:query_latency_slo:5m
          expr: >
            1 - (
              sum(rate(http_server_request_duration_seconds_bucket{route="/query",le="3"}[5m]))
              /
              clamp_min(sum(rate(http_server_request_duration_seconds_count{route="/query"}[5m])), 1)
            )

        - record: graphrag:ingestion_success_rate:5m
          expr: >
            1 - (
              sum(rate(http_server_request_duration_seconds_count{route="/ingest",status_code=~"5.."}[5m]))
              /
              clamp_min(sum(rate(http_server_request_duration_seconds_count{route="/ingest"}[5m])), 1)
            )

    - name: graphrag.slo.alerts
      rules:
        - alert: QueryAvailabilityBurnRateHigh
          expr: graphrag:query_success_rate:5m < 0.995
          for: 5m
          labels:
            severity: warning
            channel: slack
          annotations:
            summary: "Query SLO burn rate: availability below 99.5% over 5m"
            description: "Success rate is {{ $value | humanizePercentage }}"

        - alert: QueryLatencyBurnRateHigh
          expr: graphrag:query_latency_slo:5m > 0.01
          for: 5m
          labels:
            severity: warning
            channel: slack
          annotations:
            summary: "Query SLO burn rate: >1% of queries exceeding 3s latency"
            description: "Latency SLO violation rate: {{ $value | humanizePercentage }}"

    - name: graphrag.ingestion
      rules:
        - alert: HighDLQRate
          expr: rate(ingestion_dlq_routed_total[5m]) > 0.167
          for: 1m
          labels:
            severity: critical
            channel: pagerduty
          annotations:
            summary: "DLQ routing rate exceeds 10/min"
            description: >
              DLQ rate is {{ $value | humanize }}/s
              ({{ $value | humanizePercentage }} of threshold).
              Check ingestion worker logs for processing failures.

        - alert: DLQSinkFailure
          expr: rate(ingestion_dlq_sink_error_total[5m]) > 0
          for: 1m
          labels:
            severity: critical
            channel: pagerduty
          annotations:
            summary: "DLQ sink publish failing — messages at risk of loss"
            description: >
              DLQ sink publish error rate is {{ $value | humanize }}/s.
              Messages that fail processing AND fail DLQ publish are silently lost.
              Investigate ingestion worker logs immediately.

        - alert: HighConsumerLag
          expr: ingestion_consumer_lag > 1000
          for: 60s
          labels:
            severity: warning
            channel: slack
          annotations:
            summary: "Kafka consumer lag exceeds 1000 on {{ $labels.topic }}/{{ $labels.partition }}"
            description: >
              Consumer lag is {{ $value }} messages.
              HPA should be scaling ingestion workers.

        - alert: SlowBatchProcessing
          expr: histogram_quantile(0.99, rate(ingestion_batch_duration_seconds_bucket[5m])) > 2
          for: 5m
          labels:
            severity: warning
            channel: slack
          annotations:
            summary: "p99 batch duration exceeds NFR-1 SLA (2s)"
            description: "p99 batch duration: {{ $value | humanizeDuration }}"

    - name: graphrag.query
      rules:
        - alert: SlowQueryResponse
          expr: histogram_quantile(0.99, rate(query_duration_ms_bucket[5m])) > 3000
          for: 5m
          labels:
            severity: warning
            channel: slack
          annotations:
            summary: "p99 query latency exceeds NFR-2 SLA (3s)"
            description: "p99 query latency: {{ $value }}ms"

    - name: graphrag.embedding
      rules:
        - alert: EmbeddingFallbackRateHigh
          expr: rate(embedding_fallback_total[5m]) > 0
          for: 5m
          labels:
            severity: warning
            channel: slack
          annotations:
            summary: "Embedding fallback active — vector search degraded to fulltext"
            description: >
              Embedding fallback rate is {{ $value | humanize }}/s.
              The system is silently degrading to fulltext search.
              Check embedding provider health (OpenAI API status, circuit breaker state).

    - name: graphrag.infrastructure
      rules:
        - alert: Neo4jDown
          expr: up{job="neo4j"} == 0
          for: 1m
          labels:
            severity: critical
            channel: pagerduty
          annotations:
            summary: "Neo4j instance is down"

        - alert: KafkaBrokerDown
          expr: count(up{app="kafka"} == 1) < 2
          for: 2m
          labels:
            severity: critical
            channel: pagerduty
          annotations:
            summary: "Fewer than 2 Kafka brokers are healthy"

        - alert: OrchestratorHighErrorRate
          expr: >
            sum(rate(http_server_request_duration_seconds_count{status_code=~"5.."}[5m]))
            /
            sum(rate(http_server_request_duration_seconds_count[5m]))
            > 0.05
          for: 5m
          labels:
            severity: warning
            channel: slack
          annotations:
            summary: "Orchestrator 5xx rate exceeds 5%"
