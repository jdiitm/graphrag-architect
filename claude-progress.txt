## 2026-02-22: FR-3 Neo4j Knowledge Graph Persistence — Green Phase Complete

### Completed
- Added Neo4jConfig frozen dataclass to orchestrator/app/config.py (uri, username, password from env vars)
- Created orchestrator/app/neo4j_client.py: GraphRepository class with:
  - cypher_op_for_entity() — dispatches Pydantic model to type-specific Cypher MERGE query + params
  - 8 Cypher generators: ServiceNode, DatabaseNode, KafkaTopicNode, K8sDeploymentNode, CallsEdge, ProducesEdge, ConsumesEdge, DeployedInEdge
  - _partition_entities() — separates nodes from edges for ordered execution
  - GraphRepository.commit_topology() — async, opens session, execute_write in single transaction
  - GraphRepository._merge_all() — static transaction function, merges nodes first then edges
- Updated orchestrator/app/graph_builder.py: commit_to_neo4j is now async, creates AsyncGraphDatabase driver, delegates to GraphRepository
- TDD: 15 new tests written and passing (Cypher gen x10, commit_topology x2, empty x1, rollback x1, idempotent x1)

### Architecture Decisions
- Cypher MERGE on unique-constrained key properties (Service.id, KafkaTopic.name, etc.) for idempotency
- Single transaction per batch: nodes merged first, then edges (edges depend on MATCH of existing nodes)
- Entity dispatch via dict mapping type -> generator function (avoids if/elif chain)
- Driver injected into GraphRepository for testability (constructor injection)
- Error handling: catches Neo4jError and OSError (connection failures), returns commit_status="failed"

### Quality Gates
- Pylint: 10/10
- Python tests: 26/26 passing (15 new + 11 existing)
- Go tests: 8/8 passing (no regressions)
- Total: 34/34 passing, 0 regressions

### PR Status
- Branch: feat/neo4j-persistence — PR raised, awaiting human review

### Unresolved
- load_workspace_files node is still a stub
- parse_k8s_and_kafka_manifests node is still a stub
- validate_extracted_schema and fix_extraction_errors are still stubs (FR-5)
- No integration test with real Neo4j (requires running instance)

### Next Step
- Implement FR-5: Schema Validation with Correction Loop (validate_extracted_schema + fix_extraction_errors)
- OR implement load_workspace_files to recursively read a directory into raw_files

---

## 2026-02-21: parse_go_and_python_services — Green Phase Complete

### Completed
- Created orchestrator/app/config.py: ExtractionConfig frozen dataclass (GOOGLE_API_KEY, model_name, concurrency, token budget, retry params)
- Added ServiceExtractionResult to orchestrator/app/extraction_models.py (services + calls contract)
- Created orchestrator/app/llm_extraction.py: ServiceExtractor class with:
  - filter_source_files() — static, keeps only .go/.py
  - batch_by_token_budget() — static generator, greedy bin-packing by estimated tokens
  - extract_batch() — async, builds system+human prompt, calls chain (ChatGoogleGenerativeAI.with_structured_output)
  - extract_all() — async orchestrator: filter → batch → semaphore-gated gather → deduplicate by ServiceNode.id
  - Retry: tenacity exponential backoff on google.api_core ResourceExhausted/ServiceUnavailable
- Updated orchestrator/app/graph_builder.py: parse_go_and_python_services is now async, wired to ServiceExtractor
- TDD: 11 tests written and passing (filter x3, batch x3, extract_batch x2, extract_all x3)
- Infrastructure: .venv created, all deps + pytest + pytest-asyncio installed, google-api-core added

### Architecture Decisions
- LLM: Gemini via langchain-google-genai (Anthropic access is Cursor-proxied only, no standalone API key)
- Default model: gemini-2.0-flash (fast extraction), overridable to gemini-2.5-pro via EXTRACTION_MODEL env var
- self.chain = structured_llm.ainvoke (bound method) — allows direct AsyncMock in tests
- Token estimation: len(content) // 4

### Unresolved
- load_workspace_files node is still a stub
- parse_k8s_and_kafka_manifests node is still a stub
- No integration test with real LLM (requires GOOGLE_API_KEY)
- CallsEdge deduplication not yet implemented (only ServiceNode.id dedup)

### Next Step
- Implement load_workspace_files to recursively read a directory into raw_files
- OR add integration test that calls Gemini with the Go/Python fixtures to validate prompt quality

---

## 2026-02-21: Go Kafka Ingestion Worker Pool — Green Phase Complete

### Completed
- Initialized architecture_state.md with full system design (Go workers + Python orchestrator + Kafka + Neo4j)
- Created workers/ingestion/ Go module (github.com/jdiitm/graphrag-architect/workers/ingestion)
- Created internal/domain/job.go: Job and Result value types
- Created internal/processor/processor.go: DocumentProcessor interface
- Created internal/dispatcher/dispatcher.go: Dispatcher with concurrent worker pool
  - Run(ctx) launches NumWorkers goroutines reading from shared jobs channel
  - processWithRetry() retries up to MaxRetries times, checks ctx on each failure
  - Failed jobs (after max retries) routed to DLQ channel
  - Graceful shutdown: context cancellation + sync.WaitGroup ensures in-flight jobs complete
- Created internal/dlq/handler.go: DLQ Handler
  - Run(ctx) drains results from channel, forwards each to DeadLetterSink interface
  - Exits on context cancellation or closed channel
- TDD Red-Green cycle complete: 8 tests written and passing
  - Dispatcher: 6 tests (process, concurrency, retry, DLQ routing, graceful shutdown, drain)
  - DLQ Handler: 2 tests (forward to sink, stop on closed channel)

### Architecture Decisions
- Worker pool pattern: N goroutines sharing a single buffered jobs channel (fan-out)
- Retry is synchronous within each worker (no sleep, no backoff at dispatcher level)
- DLQ is a buffered channel bridged by a separate Handler goroutine to a DeadLetterSink interface
- Kafka library: franz-go selected (not yet imported — will be wired in consumer phase)
- No external dependencies yet — pure stdlib for dispatcher and DLQ

### Full Test Suite Status
- Python: 11/11 passing (orchestrator/tests/test_service_extractor.py)
- Go: 8/8 passing (dispatcher x6, dlq x2)
- Total: 19/19 passing, 0 regressions

### Unresolved
- cmd/main.go entry point not yet created (signal handling, Kafka consumer wiring)
- KafkaConsumer (franz-go) not yet integrated
- ForwardingProcessor (HTTP POST to Python orchestrator) not yet implemented
- KafkaDLQSink (publish to raw-documents.dlq topic) not yet implemented

### Next Step
- Wire cmd/main.go with signal handling, KafkaConsumer → Dispatcher → DLQ pipeline
- OR implement ForwardingProcessor that HTTP POSTs to the Python orchestrator
