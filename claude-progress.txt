## Completed: Fix Manifest Entity Loss + HMAC Token Verification (H-1, H-2, M-10)

Branch: `feat/fix-manifest-loss-and-token-verification`
Quality Gates: Pylint 10/10, Python 323/323, Go 49/49

### Audit Findings Addressed
- **H-2** — fix_extraction_errors silently discards manifest entities (K8sDeploymentNode, KafkaTopicNode)
- **H-1** — No cryptographic token verification in RBAC authentication
- **M-10** — No test coverage for manifest entity preservation in fix cycle

### Changes
1. H-2: Modified `fix_extraction_errors` in `graph_builder.py` to filter existing
   `extracted_nodes` for manifest-derived entities (K8sDeploymentNode, KafkaTopicNode)
   and merge them with re-extracted LLM entities (services + calls). Previously the
   function replaced all extracted_nodes, silently dropping manifest entities.
2. H-1: Added HMAC-SHA256 token verification to `access_control.py`:
   - `InvalidTokenError` exception for forged/tampered/unsigned tokens
   - `sign_token(payload, secret)` utility for creating valid tokens
   - `_verify_signature(token, secret)` internal verification function
   - `SecurityPrincipal.from_header` now accepts optional `token_secret` parameter
   - When `AUTH_TOKEN_SECRET` env var is set, all non-empty tokens must be HMAC-signed
   - When unset (dev mode), backwards-compatible unsigned token parsing
3. Added `AuthConfig` frozen dataclass to `config.py` with `from_env()` classmethod
4. Updated `query_engine.py` `_build_acl_filter` to load auth config and pass secret
5. Updated `main.py` to catch `InvalidTokenError` and return HTTP 401
6. 15 new tests: 3 manifest preservation, 3 sign_token, 8 HMAC verification, 1 API 401

### Architecture Decision: HMAC vs JWT
Chose HMAC-SHA256 over JWT for token verification because:
- Zero additional dependencies (stdlib hashlib + hmac)
- Simpler token format matching existing comma-separated claims
- Constant-time comparison via hmac.compare_digest
- Production deployments set AUTH_TOKEN_SECRET; dev environments skip verification

---

## Completed: DLQ Sink Failure Alert Rule (Audit 4.1)

Branch: `feat/dlq-sink-failure-alert`
Quality Gates: Pylint 10/10, Python 308/308, Go 56/56

### Audit Finding Addressed
4.1 — No alerting rule for `ingestion_dlq_sink_error_total` in `infrastructure/k8s/alerting.yaml`.
The Go DLQ handler records a Prometheus counter on sink.Send() failures, but no
PrometheusRule fired an alert. In a double-fault scenario (processing fails, then
DLQ publish also fails), messages were silently lost with no automated notification.

### Changes
- Added `DLQSinkFailure` PrometheusRule to `infrastructure/k8s/alerting.yaml`
  targeting `rate(ingestion_dlq_sink_error_total[5m]) > 0` with severity=critical
  and channel=pagerduty, fires after 1m sustained
- Added `orchestrator/tests/test_alerting_rules.py` with 9 tests:
  7 tests for DLQSinkFailure alert properties (existence, metric, severity,
  channel, for-duration, summary annotation, description annotation)
  2 tests for DLQ metric coverage (both `ingestion_dlq_routed_total` and
  `ingestion_dlq_sink_error_total` have corresponding alerting rules)

### Architecture Decision: Infrastructure Validation Tests
Added Python tests that parse alerting.yaml and validate alert rule completeness.
This prevents future observability gaps from regressing — any new DLQ-related
metric must have a corresponding alert rule to pass the test suite.

---

## Completed: Cypher Allowlist + Prometheus Scraping (Audit 3.1 + 3.2)

Branch: `feat/cypher-allowlist-and-prometheus-scraping`
Quality Gates: Pylint 10/10, Python 299/299, Go 56/56

### Audit Findings Addressed

1. **Audit 3.1** — Cypher validator does not block APOC procedure calls or LOAD CSV
   - Replaced narrow `CALL {` subquery blocklist with general `CALL` procedure blocker
   - Added `ALLOWED_PROCEDURES` frozenset allowlist for safe built-in procedures
     (`db.index.fulltext.queryNodes`, `db.labels`, `db.relationshipTypes`, etc.)
   - Added `LOAD CSV` blocker regex
   - All non-allowlisted `CALL proc(...)` patterns now raise `CypherValidationError`
   - Closes the APOC file exfiltration vector (`apoc.load.csv`, `apoc.load.json`, etc.)
   - 16 new tests (6 APOC rejection, 3 LOAD CSV rejection, 7 allowlist verification)

2. **Audit 3.2** — Prometheus cannot scrape orchestrator, Kafka, or Neo4j
   - Added `prometheus.io/scrape`, `prometheus.io/port`, `prometheus.io/path` annotations
     to orchestrator deployment pod template (port 8000)
   - Added Prometheus annotations to Neo4j StatefulSet pod template (port 7474)
   - Added three NetworkPolicy resources permitting monitoring namespace ingress:
     `allow-orchestrator-metrics` (port 8000), `allow-kafka-metrics` (port 5556),
     `allow-neo4j-metrics` (port 7474)
   - All four previously non-functional alerting rules now have reachable scrape targets

### Architecture Decision: Allowlist vs Blocklist
Switched from blocklist (block specific dangerous patterns) to allowlist (block all
procedures by default, explicitly permit known-safe ones). This is defense-in-depth:
any future Neo4j plugin or procedure is blocked until explicitly allowed.

---

## Completed: Neo4j Driver Pool + RBAC Integration (HIGH-01 + HIGH-02)

Branch: `feat/neo4j-pool-and-rbac-integration`
Quality Gates: Pylint 10/10, Python 283/283, Go 55/55

### Audit Findings Addressed

1. **HIGH-01** — RBAC module implemented but not integrated into API endpoints
   - Added `authorization` field to `QueryState` TypedDict
   - `/query` endpoint now extracts `Authorization` HTTP header via FastAPI `Header()`
   - All four retrieval functions (vector, single_hop, cypher, hybrid) apply
     `CypherPermissionFilter.inject_into_cypher()` with merged ACL params
   - Admin principals bypass all ACL filtering
   - Anonymous callers (no header) receive public-only filter
   - 7 new RBAC integration tests

2. **HIGH-02** — Neo4j driver created and destroyed per-request in query engine
   - New module `neo4j_pool.py` with `init_driver()`, `close_driver()`, `get_driver()`
   - Driver initialized once during FastAPI lifespan startup, closed on shutdown
   - `query_engine.py` and `graph_builder.py` both use pooled singleton
   - Eliminates TCP+Bolt handshake cost per query under load
   - 7 new pool lifecycle tests

### Architecture Decision: Mutable Container Pattern
Used `_state: dict[str, Optional[AsyncDriver]]` for module-level singleton
(matching existing `_STATE` pattern in `observability.py`) to avoid
`global` statement which would require inline pylint suppression.

---

## Completed: Prometheus Metrics Export (HIGH-1 + MINOR-1)

Branch: `feat/prometheus-metrics-export`
Quality Gates: Pylint 10/10, Python 269/269, Go 49/49

### Audit Findings Addressed

1. **HIGH-1** — Python Prometheus metrics recorded but never exported
   - Added `configure_metrics()` to `observability.py` using `PrometheusMetricReader` + `MeterProvider`
   - Added `GET /metrics` endpoint to `main.py` serving Prometheus exposition format via `prometheus_client`
   - Wired `configure_metrics()` into FastAPI lifespan alongside `configure_telemetry()`
   - Removed redundant `unit="ms"` from histogram definitions to avoid double-suffixed Prometheus names
   - All four OTel histograms (ingestion, llm_extraction, neo4j_transaction, query duration) now exported
   - FastAPI auto-instrumentation metrics (http_server_request_duration_seconds) also exported
   - Both alert rules (SlowQueryResponse, OrchestratorHighErrorRate) now reference scrapable metrics
   - 9 new tests: configure_metrics unit tests, /metrics endpoint tests, histogram export integration tests

2. **MINOR-1** — Architecture doc drift
   - Added `cypher_validator.py` and `ingest_models.py` to module listing in `01_SYSTEM_DESIGN.md`
   - Removed duplicate `config.py` entry
   - Updated observability.py description and test count

---

## Completed: DLQ Error Handling + Infrastructure Fixes

Branch: `feat/dlq-error-handling-and-infra-fixes`
Quality Gates: Pylint 10/10, Python 260/260, Go 49/49

### Audit Findings Addressed

1. **HIGH 3.1** — DLQ handler silently discards sink errors (data loss vector)
   - Extended PipelineObserver with RecordDLQSinkError()
   - Added `ingestion_dlq_sink_error_total` Prometheus counter
   - Modified Handler to log errors via slog and emit metrics via observer
   - Used functional options pattern (WithObserver, WithLogger) matching dispatcher/consumer
   - Wired observer into production main.go
   - 4 new tests: error metric recording, continues after error, no metric on success, logger option

2. **HIGH 3.2** — HPA consumer lag metric name and label mismatch
   - Fixed hpa.yaml: metric name `ingestion_consumer_lag` (was `_total`)
   - Removed incorrect `consumer_group` label selector (metric uses topic/partition labels)

3. **Minor 4.1** — Network policies block OTLP telemetry export
   - Added port 4317 egress to orchestrator network policy (otel-collector pod)
   - Added port 4317 egress to ingestion-worker network policy (otel-collector pod)

4. **Minor 4.2** — Inline pylint suppression violates CLAUDE.md
   - Moved broad-except suppression to pyproject.toml (W0718) with justification comment
   - Removed inline `# pylint: disable=broad-except` from observability.py

---

## Project Status: Feature-Complete

All functional requirements (FR-1 through FR-8) and non-functional requirements
have been implemented, tested, and merged to main.

---

## Implementation Summary

### Phase 1: Core Pipeline

| Feature | Module | Status |
|---|---|---|
| FR-1: Async Kafka Ingestion | `workers/ingestion/` | Complete |
| FR-2: LLM Entity Extraction | `orchestrator/app/llm_extraction.py` | Complete |
| FR-3: Neo4j Persistence | `orchestrator/app/neo4j_client.py` | Complete |
| FR-4: Hybrid Query Routing | `orchestrator/app/query_engine.py` | Complete |
| FR-5: Schema Validation Loop | `orchestrator/app/schema_validation.py` | Complete |
| FR-6: DLQ Fault Tolerance | `workers/ingestion/internal/dlq/` | Complete |

### Phase 2: Cross-Cutting Concerns

| Feature | Module | Status |
|---|---|---|
| FR-7: Access Control | `orchestrator/app/access_control.py` | Complete |
| FR-8: Observability (Python) | `orchestrator/app/observability.py` | Complete |
| FR-8: Observability (Go) | `workers/ingestion/internal/telemetry/` | Complete |

### Phase 3: NFR Hardening & Deployment

| Item | Module | Status |
|---|---|---|
| Circuit Breaker (NFR-6) | `orchestrator/app/circuit_breaker.py` | Complete |
| Error-Aware Sampling (NFR-8) | `orchestrator/app/observability.py` | Complete |
| Prometheus Metrics Wiring | `workers/ingestion/internal/metrics/` | Complete |
| Integration Tests | `orchestrator/tests/test_integration.py` | Complete |
| CI/CD Pipeline | `.github/workflows/ci.yml` | Complete |
| Dockerfiles | `orchestrator/Dockerfile`, `workers/ingestion/Dockerfile` | Complete |
| K8s Manifests | `infrastructure/k8s/` | Complete |
| StatefulSets (Neo4j, Kafka) | `infrastructure/k8s/` | Complete |
| Ingress, NetworkPolicies | `infrastructure/k8s/` | Complete |
| Alerting (PrometheusRule) | `infrastructure/k8s/alerting.yaml` | Complete |

---

## Architecture

- Python orchestrator: FastAPI + LangGraph (ingestion DAG + query DAG)
- Go ingestion workers: Kafka consumer, dispatcher, forwarding processor, DLQ
- Neo4j 5.26: Knowledge graph with 5 constraints, 3 indexes, fulltext search
- Apache Kafka 3.9: KRaft mode, at-least-once delivery
- OpenTelemetry: Distributed tracing across Go -> HTTP -> Python boundary
- Prometheus: Consumer lag, batch duration, DLQ rate, jobs processed metrics

---

## Quality Gates

All gates pass before every push:
1. Pylint: 10/10
2. Python tests: all passing
3. Go tests: all passing
