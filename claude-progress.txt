## Completed: DLQ Deadlock Remediation (HIGH — Audit 3.1)

Branch: `feat/dlq-deadlock-remediation`
Quality Gates: Pylint 10/10, Python 398/398, Go 82/82

### Audit Finding Addressed
Consumer pipeline deadlock when DLQ Kafka sink becomes unreachable. Three
root causes: missing ack timeout, no DLQ fallback sink, liveness probe blind spot.

### Changes
1. `cmd/main.go`: Configured `consumer.WithAckTimeout(30s)` (env: ACK_TIMEOUT_SECONDS),
   file-based DLQ fallback via `dlq.WithFallback()` (env: DLQ_FALLBACK_PATH),
   and dedicated `/healthz` liveness endpoint
2. `internal/dlq/file_sink.go`: File-based DLQ fallback sink implementing `DeadLetterSink`.
   Writes JSONL records with full error metadata. Thread-safe, creates parent directories.
3. `internal/healthz/checker.go`: Dedicated liveness endpoint checking consumer health via
   `LastBatchTime()`. Returns 503 when no activity recorded or activity exceeds threshold
   (default 45s, below Kafka session.timeout.ms). K8s liveness probe should target this
   instead of `/metrics`.
4. `internal/metrics/metrics.go`: Added `LastBatchTime()` tracking via `sync/atomic.Value`,
   updated on every `RecordBatchDuration()` call (post-commit).

### New Tests: 13 (all Go)
- 4 file sink tests (write, append, parent dirs, interface compliance)
- 6 healthz checker tests (healthy, no activity, stale, default threshold, custom threshold, JSON)
- 3 metrics LastBatchTime tests (zero, updated, subsequent)

---

## Completed: Query Path require_tokens Fail-Closed Fix (LOW-001)

Branch: `fix/query-require-tokens-fail-closed`
Quality Gates: Pylint 10/10, Python 398/398, Go 69/69

### Audit Finding Addressed

1. **LOW-001** — `/query` path in `query_engine.py:43` did not enforce `require_tokens`
   flag. When `AUTH_REQUIRE_TOKENS=true` and `AUTH_TOKEN_SECRET` is empty, `/ingest`
   correctly returned 503 (fail-closed) but `/query` silently accepted unsigned tokens
   (fail-open). Fix: added `require_tokens` check to `_build_acl_filter()` in
   `query_engine.py`, mirroring the ingest path. When `require_tokens=true` and
   `token_secret` is empty, raises `AuthConfigurationError` resulting in HTTP 503.

### Changes
1. `orchestrator/app/access_control.py`: Added `AuthConfigurationError` exception class
2. `orchestrator/app/query_engine.py`: Added `require_tokens` check to `_build_acl_filter()`
3. `orchestrator/app/main.py`: Catch `AuthConfigurationError` in `/query` endpoint, return 503
4. `orchestrator/tests/test_query_engine.py`: 3 new tests for `_build_acl_filter` behavior
5. `orchestrator/tests/test_query_api.py`: 1 new test for 503 API response

### New Tests: 4 total (4 Python)

---

## Completed: DLQ Silent Loss Fix + Infra Gaps (HIGH-001, HIGH-002, LOW-001)

Branch: `fix/dlq-silent-loss-and-infra-gaps`
Quality Gates: Pylint 10/10, Python 394/394, Go 69/69

### Audit Findings Addressed

1. **HIGH-001** — DLQ handler `processResult` unconditionally closed Done channel
   via `defer close(result.Done)` even when sink fails and no fallback exists.
   This allowed silent message loss: dispatcher unblocked, offset committed,
   but message neither processed nor persisted in DLQ.
   Fix: Removed unconditional defer. Done now only closed on confirmed sink success
   or confirmed fallback success. On failure without fallback (or fallback failure),
   Done stays open so dispatcher blocks and offset is NOT committed.
   Existing test `TestDLQHandlerClosesDoneAfterRetriesExhausted` encoded the buggy
   behavior — replaced with `TestDLQHandlerDoneNotClosedWhenSinkFailsNoFallback`.

2. **HIGH-002** — K8s kafka-statefulset.yaml missing KAFKA_LISTENER_SECURITY_PROTOCOL_MAP.
   Fix: Added `CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT` matching docker-compose.yml.

3. **LOW-001** — secrets.yaml missing AUTH_TOKEN_SECRET placeholder.
   Fix: Added `AUTH_TOKEN_SECRET: "REPLACE_ME"` to stringData.

### Changes
1. `workers/ingestion/internal/dlq/handler.go`: Removed unconditional `defer close(result.Done)`,
   added `closeDone` helper called only on confirmed sink/fallback success
2. `workers/ingestion/internal/dlq/handler_test.go`: Fixed spec-error test, added 2 new tests
3. `infrastructure/k8s/kafka-statefulset.yaml`: Added KAFKA_LISTENER_SECURITY_PROTOCOL_MAP
4. `infrastructure/k8s/secrets.yaml`: Added AUTH_TOKEN_SECRET placeholder
5. `orchestrator/tests/test_infra_security.py`: 5 new tests (3 protocol map, 2 secrets)

### New Tests: 7 total (2 Go, 5 Python)

---

## Completed: Kafka Infrastructure Fixes (CRITICAL-001, CRITICAL-002)

Branch: `feat/kafka-infrastructure-fixes`
Quality Gates: Pylint 10/10, Python 389/389, Go 68/68

### Audit Findings Addressed

1. **CRITICAL-001**: Missing Kafka egress NetworkPolicy
   - Added `allow-kafka-egress` NetworkPolicy to `network-policies.yaml`
   - Permits Kafka-to-Kafka egress on port 9092 (inter-broker replication)
   - Permits Kafka-to-Kafka egress on port 9093 (KRaft controller quorum)
   - Permits DNS resolution egress (port 53 UDP/TCP)
   - Without this policy, the deny-all default blocked all inter-broker
     communication, making KRaft quorum formation impossible

2. **CRITICAL-002**: Missing CLUSTER_ID in kafka-statefulset.yaml
   - Added `CLUSTER_ID=MkU3OEVBNTcwNTJENDM2Qk` env var to kafka container
   - Matches the value in docker-compose.yml for environment consistency
   - Without this, each KRaft broker auto-generated its own cluster ID,
     forming 3 separate single-node clusters instead of one 3-node cluster

### Changes
1. `infrastructure/k8s/network-policies.yaml`: Added allow-kafka-egress NetworkPolicy
2. `infrastructure/k8s/kafka-statefulset.yaml`: Added CLUSTER_ID env var
3. `orchestrator/tests/test_infra_security.py`: 8 new tests (5 egress policy, 3 cluster ID)

### Architecture Decision: Minimal Egress Scope
Kafka egress policy only permits traffic to other kafka pods (app: kafka)
on ports 9092/9093 and DNS resolution. No broader network access is granted,
maintaining the principle of least privilege from the deny-all baseline.

---

## Completed: Performance and Consumer Resilience Hardening

Branch: `feat/performance-and-consumer-resilience`
Quality Gates: Pylint 10/10, Python 360/360, Go 64/64

### Deep Audit Findings Addressed (3.1, 3.2)

- **3.1 (HIGH)**: Converted workspace_loader from synchronous full-load to chunked
  generator. `load_directory_chunked()` yields batches with configurable chunk_size
  and optional max_total_bytes cap. Prevents OOM on large monorepos. Original
  `load_directory()` preserved as backward-compatible thin wrapper.
  Files: workspace_loader.py, test_workspace_loader.py

- **3.2 (MEDIUM)**: Added configurable `WithAckTimeout` to Go consumer. When a batch
  ack exceeds the timeout, the consumer returns `ErrAckTimeout` instead of blocking
  indefinitely. Prevents session.timeout.ms from being exceeded by stalled jobs,
  avoiding Kafka consumer group rebalances. Default is no timeout (backward compat).
  Files: consumer.go, consumer_test.go

---

## Completed: Audit Mitigation — DLQ Durability, Token TTL, Ingest Auth, E2E Tests

Branch: `fix/audit-mitigations`
Quality Gates: Pylint 10/10, Python 354/354, Go 61/61

### Critical Fixes Applied (TDD Red-Green-Refactor)

- **B3 (DLQ Data Loss — CRITICAL)**: Dispatcher now waits for DLQ handler confirmation
  before acking. Result.Done channel blocks offset commit until sink write is confirmed.
  DLQ handler retries sink up to 3x with configurable delay and supports a fallback sink.
  Files: domain/job.go, dispatcher/dispatcher.go, dlq/handler.go

- **B5 (Token TTL — HIGH)**: sign_token() now embeds iat/exp epoch claims in the token
  payload. _verify_signature() rejects expired tokens. Default TTL: 3600s (1 hour),
  configurable via AUTH_TOKEN_TTL env var.
  Files: access_control.py, config.py

- **B2 (Ingest Auth — HIGH)**: /ingest endpoint now accepts optional Authorization
  header. When AUTH_TOKEN_SECRET is configured, unauthenticated requests return 401.
  Go ForwardingProcessor supports WithAuthToken() option, reads AUTH_TOKEN from env.
  Files: main.py, forwarding.go, cmd/main.go

- **B4 (E2E Test — MEDIUM)**: Added TestHTTPThroughDAGIntegration exercising
  POST /ingest through the real LangGraph DAG (external deps mocked). Proves the
  HTTP endpoint → load_workspace → parse → validate → commit flow is end-to-end correct.
  Files: test_integration.py

### Audit Finding B1 (Silent Data Drop) — Verified NOT a bug
  load_workspace_files() at graph_builder.py:49-56 correctly preserves raw_files when
  directory_path is empty. Existing integration test at test_integration.py:102-115
  already proves this. The new E2E test provides additional HTTP-level confirmation.

---

## Completed: K8s Security Context + Neo4j Commit Error Logging (Audit M-1, M-2)

Branch: `feat/k8s-security-context-and-commit-logging`
Quality Gates: Pylint 10/10, Python 343/343, Go 49/49

### Audit Findings Addressed
- **M-1** — K8s deployments lack pod securityContext: added `runAsNonRoot: true`,
  `readOnlyRootFilesystem: true`, and `allowPrivilegeEscalation: false` to both
  orchestrator and ingestion-worker container specs. This provides defense-in-depth
  beyond the Dockerfile `USER appuser` directive, preventing root execution even
  if the deployment image is changed.
- **M-2** — Neo4j commit errors caught but not logged: added `logger.error()` call,
  OTel span error status (`StatusCode.ERROR`), and span exception recording
  (`record_exception`) to `commit_to_neo4j` in `graph_builder.py`. Operators can
  now determine the specific cause of Neo4j commit failures from logs and traces.

### Changes
1. `infrastructure/k8s/orchestrator-deployment.yaml`: Added container securityContext
2. `infrastructure/k8s/ingestion-worker-deployment.yaml`: Added container securityContext
3. `orchestrator/app/graph_builder.py`: Added logging, span error status, and exception
   recording to commit_to_neo4j error path
4. `orchestrator/tests/test_infra_security.py`: 6 new tests for securityContext properties
5. `orchestrator/tests/test_graph_builder.py`: 6 new tests for error logging and span recording

### Architecture Decision: Defense-in-Depth
K8s securityContext enforces non-root execution at the orchestration layer regardless of
image content. Combined with existing Dockerfile USER directive and network policies,
this creates three independent layers of defense. The error logging change ensures full
observability of Neo4j commit failures through both structured logs and distributed traces.

---

## Completed: Infrastructure Security Hardening (M-1, M-2)

Branch: `feat/fix-infra-security-minor-findings`
Quality Gates: Pylint 10/10, Python 331/331, Go 49/49

### Audit Findings Addressed
- **M-1** — Cross-namespace NetworkPolicy: replaced `namespaceSelector: {}`
  (wildcard) with explicit `kubernetes.io/metadata.name: graphrag` label match
  in the `allow-orchestrator-ingress` ingestion-worker ingress rule. This
  prevents pods in other namespaces from spoofing the `app: ingestion-worker`
  label to reach the orchestrator.
- **M-2** — Docker Compose APOC file access: removed `NEO4J_apoc_export_file_enabled`,
  `NEO4J_apoc_import_file_enabled`, and `NEO4J_apoc_import_file_use__neo4j__config`
  from the dev Neo4j container. No APOC file procedures are used by the application.

### Changes
1. `infrastructure/k8s/network-policies.yaml`: Added explicit graphrag namespace
   selector to ingestion-worker ingress rule (was wildcard `{}`)
2. `infrastructure/docker-compose.yml`: Removed 3 APOC file-access env vars
3. `orchestrator/tests/test_infra_security.py`: 8 new tests (3 NetworkPolicy
   namespace restriction, 5 Docker Compose APOC file access prevention)

### Architecture Decision: Infrastructure-as-Code Testing
Added Python tests that parse infrastructure YAML files and validate security
properties. This pattern (already used by `test_alerting_rules.py`) prevents
security regressions by encoding infrastructure security requirements as
executable specifications.

---

## Completed: Fix Manifest Entity Loss + HMAC Token Verification (H-1, H-2, M-10)

Branch: `feat/fix-manifest-loss-and-token-verification`
Quality Gates: Pylint 10/10, Python 323/323, Go 49/49

### Audit Findings Addressed
- **H-2** — fix_extraction_errors silently discards manifest entities (K8sDeploymentNode, KafkaTopicNode)
- **H-1** — No cryptographic token verification in RBAC authentication
- **M-10** — No test coverage for manifest entity preservation in fix cycle

### Changes
1. H-2: Modified `fix_extraction_errors` in `graph_builder.py` to filter existing
   `extracted_nodes` for manifest-derived entities (K8sDeploymentNode, KafkaTopicNode)
   and merge them with re-extracted LLM entities (services + calls). Previously the
   function replaced all extracted_nodes, silently dropping manifest entities.
2. H-1: Added HMAC-SHA256 token verification to `access_control.py`:
   - `InvalidTokenError` exception for forged/tampered/unsigned tokens
   - `sign_token(payload, secret)` utility for creating valid tokens
   - `_verify_signature(token, secret)` internal verification function
   - `SecurityPrincipal.from_header` now accepts optional `token_secret` parameter
   - When `AUTH_TOKEN_SECRET` env var is set, all non-empty tokens must be HMAC-signed
   - When unset (dev mode), backwards-compatible unsigned token parsing
3. Added `AuthConfig` frozen dataclass to `config.py` with `from_env()` classmethod
4. Updated `query_engine.py` `_build_acl_filter` to load auth config and pass secret
5. Updated `main.py` to catch `InvalidTokenError` and return HTTP 401
6. 15 new tests: 3 manifest preservation, 3 sign_token, 8 HMAC verification, 1 API 401

### Architecture Decision: HMAC vs JWT
Chose HMAC-SHA256 over JWT for token verification because:
- Zero additional dependencies (stdlib hashlib + hmac)
- Simpler token format matching existing comma-separated claims
- Constant-time comparison via hmac.compare_digest
- Production deployments set AUTH_TOKEN_SECRET; dev environments skip verification

---

## Completed: DLQ Sink Failure Alert Rule (Audit 4.1)

Branch: `feat/dlq-sink-failure-alert`
Quality Gates: Pylint 10/10, Python 308/308, Go 56/56

### Audit Finding Addressed
4.1 — No alerting rule for `ingestion_dlq_sink_error_total` in `infrastructure/k8s/alerting.yaml`.
The Go DLQ handler records a Prometheus counter on sink.Send() failures, but no
PrometheusRule fired an alert. In a double-fault scenario (processing fails, then
DLQ publish also fails), messages were silently lost with no automated notification.

### Changes
- Added `DLQSinkFailure` PrometheusRule to `infrastructure/k8s/alerting.yaml`
  targeting `rate(ingestion_dlq_sink_error_total[5m]) > 0` with severity=critical
  and channel=pagerduty, fires after 1m sustained
- Added `orchestrator/tests/test_alerting_rules.py` with 9 tests:
  7 tests for DLQSinkFailure alert properties (existence, metric, severity,
  channel, for-duration, summary annotation, description annotation)
  2 tests for DLQ metric coverage (both `ingestion_dlq_routed_total` and
  `ingestion_dlq_sink_error_total` have corresponding alerting rules)

### Architecture Decision: Infrastructure Validation Tests
Added Python tests that parse alerting.yaml and validate alert rule completeness.
This prevents future observability gaps from regressing — any new DLQ-related
metric must have a corresponding alert rule to pass the test suite.

---

## Completed: Cypher Allowlist + Prometheus Scraping (Audit 3.1 + 3.2)

Branch: `feat/cypher-allowlist-and-prometheus-scraping`
Quality Gates: Pylint 10/10, Python 299/299, Go 56/56

### Audit Findings Addressed

1. **Audit 3.1** — Cypher validator does not block APOC procedure calls or LOAD CSV
   - Replaced narrow `CALL {` subquery blocklist with general `CALL` procedure blocker
   - Added `ALLOWED_PROCEDURES` frozenset allowlist for safe built-in procedures
     (`db.index.fulltext.queryNodes`, `db.labels`, `db.relationshipTypes`, etc.)
   - Added `LOAD CSV` blocker regex
   - All non-allowlisted `CALL proc(...)` patterns now raise `CypherValidationError`
   - Closes the APOC file exfiltration vector (`apoc.load.csv`, `apoc.load.json`, etc.)
   - 16 new tests (6 APOC rejection, 3 LOAD CSV rejection, 7 allowlist verification)

2. **Audit 3.2** — Prometheus cannot scrape orchestrator, Kafka, or Neo4j
   - Added `prometheus.io/scrape`, `prometheus.io/port`, `prometheus.io/path` annotations
     to orchestrator deployment pod template (port 8000)
   - Added Prometheus annotations to Neo4j StatefulSet pod template (port 7474)
   - Added three NetworkPolicy resources permitting monitoring namespace ingress:
     `allow-orchestrator-metrics` (port 8000), `allow-kafka-metrics` (port 5556),
     `allow-neo4j-metrics` (port 7474)
   - All four previously non-functional alerting rules now have reachable scrape targets

### Architecture Decision: Allowlist vs Blocklist
Switched from blocklist (block specific dangerous patterns) to allowlist (block all
procedures by default, explicitly permit known-safe ones). This is defense-in-depth:
any future Neo4j plugin or procedure is blocked until explicitly allowed.

---

## Completed: Neo4j Driver Pool + RBAC Integration (HIGH-01 + HIGH-02)

Branch: `feat/neo4j-pool-and-rbac-integration`
Quality Gates: Pylint 10/10, Python 283/283, Go 55/55

### Audit Findings Addressed

1. **HIGH-01** — RBAC module implemented but not integrated into API endpoints
   - Added `authorization` field to `QueryState` TypedDict
   - `/query` endpoint now extracts `Authorization` HTTP header via FastAPI `Header()`
   - All four retrieval functions (vector, single_hop, cypher, hybrid) apply
     `CypherPermissionFilter.inject_into_cypher()` with merged ACL params
   - Admin principals bypass all ACL filtering
   - Anonymous callers (no header) receive public-only filter
   - 7 new RBAC integration tests

2. **HIGH-02** — Neo4j driver created and destroyed per-request in query engine
   - New module `neo4j_pool.py` with `init_driver()`, `close_driver()`, `get_driver()`
   - Driver initialized once during FastAPI lifespan startup, closed on shutdown
   - `query_engine.py` and `graph_builder.py` both use pooled singleton
   - Eliminates TCP+Bolt handshake cost per query under load
   - 7 new pool lifecycle tests

### Architecture Decision: Mutable Container Pattern
Used `_state: dict[str, Optional[AsyncDriver]]` for module-level singleton
(matching existing `_STATE` pattern in `observability.py`) to avoid
`global` statement which would require inline pylint suppression.

---

## Completed: Prometheus Metrics Export (HIGH-1 + MINOR-1)

Branch: `feat/prometheus-metrics-export`
Quality Gates: Pylint 10/10, Python 269/269, Go 49/49

### Audit Findings Addressed

1. **HIGH-1** — Python Prometheus metrics recorded but never exported
   - Added `configure_metrics()` to `observability.py` using `PrometheusMetricReader` + `MeterProvider`
   - Added `GET /metrics` endpoint to `main.py` serving Prometheus exposition format via `prometheus_client`
   - Wired `configure_metrics()` into FastAPI lifespan alongside `configure_telemetry()`
   - Removed redundant `unit="ms"` from histogram definitions to avoid double-suffixed Prometheus names
   - All four OTel histograms (ingestion, llm_extraction, neo4j_transaction, query duration) now exported
   - FastAPI auto-instrumentation metrics (http_server_request_duration_seconds) also exported
   - Both alert rules (SlowQueryResponse, OrchestratorHighErrorRate) now reference scrapable metrics
   - 9 new tests: configure_metrics unit tests, /metrics endpoint tests, histogram export integration tests

2. **MINOR-1** — Architecture doc drift
   - Added `cypher_validator.py` and `ingest_models.py` to module listing in `01_SYSTEM_DESIGN.md`
   - Removed duplicate `config.py` entry
   - Updated observability.py description and test count

---

## Completed: DLQ Error Handling + Infrastructure Fixes

Branch: `feat/dlq-error-handling-and-infra-fixes`
Quality Gates: Pylint 10/10, Python 260/260, Go 49/49

### Audit Findings Addressed

1. **HIGH 3.1** — DLQ handler silently discards sink errors (data loss vector)
   - Extended PipelineObserver with RecordDLQSinkError()
   - Added `ingestion_dlq_sink_error_total` Prometheus counter
   - Modified Handler to log errors via slog and emit metrics via observer
   - Used functional options pattern (WithObserver, WithLogger) matching dispatcher/consumer
   - Wired observer into production main.go
   - 4 new tests: error metric recording, continues after error, no metric on success, logger option

2. **HIGH 3.2** — HPA consumer lag metric name and label mismatch
   - Fixed hpa.yaml: metric name `ingestion_consumer_lag` (was `_total`)
   - Removed incorrect `consumer_group` label selector (metric uses topic/partition labels)

3. **Minor 4.1** — Network policies block OTLP telemetry export
   - Added port 4317 egress to orchestrator network policy (otel-collector pod)
   - Added port 4317 egress to ingestion-worker network policy (otel-collector pod)

4. **Minor 4.2** — Inline pylint suppression violates CLAUDE.md
   - Moved broad-except suppression to pyproject.toml (W0718) with justification comment
   - Removed inline `# pylint: disable=broad-except` from observability.py

---

## Project Status: Feature-Complete

All functional requirements (FR-1 through FR-8) and non-functional requirements
have been implemented, tested, and merged to main.

---

## Implementation Summary

### Phase 1: Core Pipeline

| Feature | Module | Status |
|---|---|---|
| FR-1: Async Kafka Ingestion | `workers/ingestion/` | Complete |
| FR-2: LLM Entity Extraction | `orchestrator/app/llm_extraction.py` | Complete |
| FR-3: Neo4j Persistence | `orchestrator/app/neo4j_client.py` | Complete |
| FR-4: Hybrid Query Routing | `orchestrator/app/query_engine.py` | Complete |
| FR-5: Schema Validation Loop | `orchestrator/app/schema_validation.py` | Complete |
| FR-6: DLQ Fault Tolerance | `workers/ingestion/internal/dlq/` | Complete |

### Phase 2: Cross-Cutting Concerns

| Feature | Module | Status |
|---|---|---|
| FR-7: Access Control | `orchestrator/app/access_control.py` | Complete |
| FR-8: Observability (Python) | `orchestrator/app/observability.py` | Complete |
| FR-8: Observability (Go) | `workers/ingestion/internal/telemetry/` | Complete |

### Phase 3: NFR Hardening & Deployment

| Item | Module | Status |
|---|---|---|
| Circuit Breaker (NFR-6) | `orchestrator/app/circuit_breaker.py` | Complete |
| Error-Aware Sampling (NFR-8) | `orchestrator/app/observability.py` | Complete |
| Prometheus Metrics Wiring | `workers/ingestion/internal/metrics/` | Complete |
| Integration Tests | `orchestrator/tests/test_integration.py` | Complete |
| CI/CD Pipeline | `.github/workflows/ci.yml` | Complete |
| Dockerfiles | `orchestrator/Dockerfile`, `workers/ingestion/Dockerfile` | Complete |
| K8s Manifests | `infrastructure/k8s/` | Complete |
| StatefulSets (Neo4j, Kafka) | `infrastructure/k8s/` | Complete |
| Ingress, NetworkPolicies | `infrastructure/k8s/` | Complete |
| Alerting (PrometheusRule) | `infrastructure/k8s/alerting.yaml` | Complete |

---

## Architecture

- Python orchestrator: FastAPI + LangGraph (ingestion DAG + query DAG)
- Go ingestion workers: Kafka consumer, dispatcher, forwarding processor, DLQ
- Neo4j 5.26: Knowledge graph with 5 constraints, 3 indexes, fulltext search
- Apache Kafka 3.9: KRaft mode, at-least-once delivery
- OpenTelemetry: Distributed tracing across Go -> HTTP -> Python boundary
- Prometheus: Consumer lag, batch duration, DLQ rate, jobs processed metrics

---

## Quality Gates

All gates pass before every push:
1. Pylint: 10/10
2. Python tests: all passing
3. Go tests: all passing

---

## Completed: External Audit Mitigation — Full 14-Cycle Roadmap

Quality Gates: Pylint 10/10, Python 629/629, Go 109/109

### Summary

Implemented all 14 cycles across 4 waves from the External Audit Mitigation Roadmap,
addressing the 10 critical weaknesses and 5 architectural refactors identified in the
FAANG L9 deep system audit.

### Wave 1 — Security (Cycles 1–3)
- Cycle 1: Replaced custom HMAC CSV-payload signer with PyJWT (HS256). Fail-closed
  when token provided but no secret configured.
- Cycle 2: Replaced regex-based Cypher validator with token-aware parser. Write keywords
  inside string literals no longer trigger false positives.
- Cycle 3: Fixed ACL field population in manifest parser. Fallback labels (team, owner,
  app.kubernetes.io/managed-by) and namespace-from-metadata now prevent silently dropped ACLs.

### Wave 2 — Reliability (Cycles 4–7)
- Cycle 4: Added exponential backoff with jitter to Go dispatcher retry loop.
  Conditional on BaseBackoff > 0 for backward-compatible tests.
- Cycle 5: Fixed Kafka consumer head-of-line blocking with explicit session/rebalance/
  heartbeat timeouts.
- Cycle 6: DLQ poison pill deadlock prevention via configurable DLQAckTimeout in
  dispatcher. DLQ handler only closes Done channel on confirmed success.
- Cycle 7: Refactored circuit breaker to use StateStore Protocol abstraction.
  InMemoryStateStore default; Redis-backed store ready via Protocol interface.

### Wave 3 — Performance & AI (Cycles 8–10)
- Cycle 8: Streaming workspace loader with cumulative byte limit (MAX_INGEST_PAYLOAD_BYTES).
  Generator-based decoding prevents OOM on large payloads.
- Cycle 9: Real vector embeddings via OpenAI text-embedding-3-small + Neo4j HNSW index.
  Fulltext BM25 retained as graceful fallback when embeddings unavailable.
- Cycle 10: Decoupled AST extraction from LLM. Deterministic Go/Python AST extractors
  run first (confidence=1.0), LLM enriches only novel entities (confidence=0.7).
  Token budget reduced from 200k to 8k.

### Wave 4 — Infrastructure (Cycles 11–14)
- Cycle 11: Verified Kafka advertised listeners and NetworkPolicy for schema init job
  already correctly configured. No changes required.
- Cycle 12: Neo4j Enterprise/HA migration — 3 replicas, enterprise image, causal
  clustering config. Routing driver (neo4j://) as default URI scheme.
- Cycle 13: Decoupled ingestion — new KafkaForwardingProcessor writes parsed JSON
  payloads to intermediate topic (no base64). Python AsyncKafkaConsumer supports
  both legacy base64 and new JSON parsed-message formats.
- Cycle 14: Wired RAGEvaluator into query pipeline as evaluate node between synthesize
  and END. Fixed LangGraph streaming bypass — _process_chunk now delegates to compiled
  ingestion_graph.ainvoke() instead of manually orchestrating node calls.
